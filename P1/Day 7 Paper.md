==target==: new medium 

- political bias 

- factuality

  

extracted ==target== feature -> sources:

Goal: predicting the political bias and the factuality

- Task: 



##### **Compared**:

​	better than compared to predicting each of them independently

​    joint prediction.

##### **System Architecture** 

downstram task 

- feature extract 

  - what was written by the medium
  - the audience of the medium on social media
  - what was written about the medium in Wikipedia

  -> multi-model () feature set

  - text
  - speech
  - metadata

which we used to **train a classifier** to predict the political bias and the factuality of reporting of news media. 

**new medium.NELA feature** 

##### Feature Description

feature used to model content generated by new media 

Given target news medium, 

- num of articles 

- linguistic feature using News Landscape (NELA) toolkit named as NELA features
  - averaged (arithmetic averaging) the NELA features for the individual articles in order to obtain a NELA representation for a news medium
- Embedding Feature
  - encoded each article using BERT ==(Devlin et al., 2019)==
  - WordPieces 
  - averaging the word representations

**Fine-turned Bert**

- why/goal: 

- how:
  -  by training a softmax layer on top of the [CLS] output vector to predict the label (bias or factuality) of news articles that are scrapped from an external list of media to avoid overfitting. 
  - The articles’ labels are assumed to be the same as those of the media in which they are published (a form of distant supervision).
  - fake news” detection























演讲稿（填空）：

###### Datasets: The dataset we will use in this example is

###### Model: Sentence Sentiment Classification (总)

Our goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:

<img src="http://jalammar.github.io/images/distilBERT/sentiment-classifier-1.png" alt="img" style="zoom:50%;" />

Under the hood, the model is actually made up of two model.(分)

- [DistilBERT](https://medium.com/huggingface/distilbert-8cf3380435b5) processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at [HuggingFace](https://huggingface.co/). It’s a lighter and faster version of BERT that roughly matches its performance.
- The next model, a basic Logistic Regression model from scikit learn will take in the result of DistilBERT’s processing, and classify the sentence as either positive or negative (1 or 0, respectively).

The data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification.

<img src="http://jalammar.github.io/images/distilBERT/distilbert-bert-sentiment-classifier.png" alt="img" style="zoom:50%;" />

If you’ve read my previous post, [Illustrated BERT](http://jalammar.github.io/illustrated-bert/), this vector is the result of the first position (which receives the [CLS] token as input).

###### Model Training

only train the logistic regression model

For DistillBERT, we’ll use a model that’s already **pre-trained** and has a grasp on the English language

This model, however is neither trained not fine-tuned to do sentence classification. 

<img src="http://jalammar.github.io/images/distilBERT/model-training.png" alt="img" style="zoom: 50%;" />

 

###### Tutorial Overview

We will first use the trained distilBERT to **generate sentence embeddings** for 2,000 sentences.

![img](http://jalammar.github.io/images/distilBERT/bert-distilbert-tutorial-sentence-embedding.png)

We will not touch distilBERT after this step. It’s all **Scikit-Learn** from here. We do the usual train/test split on this dataset:

<img src="http://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png" alt="img" style="zoom: 67%;" />

Train/test split for the output of distilBert (model #1) creates the dataset we'll train and evaluate logistic regression on (model #2). Note that in reality, sklearn's train/test split shuffles the examples before making the split, it doesn't just take the first 75% of examples as they appear in the dataset.



Then we train the logistic regression model on the training set:

<img src="http://jalammar.github.io/images/distilBERT/bert-training-logistic-regression.png" alt="img"  />



###### How a single prediction is calculated

let’s look at how a trained model calculates its prediction.

Let’s try to classify the sentence “==a visually stunning rumination on love==”. 

- The first step is to use the BERT **tokenizer** to first split the word into tokens. 

- Then, we add the **special tokens** needed for sentence classifications (these are **[CLS]** at the first position, and **[SEP]** at the end of the sentence).

![img](http://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-1.png)

The third step the tokenizer does is to replace each token with its id from the embedding table which is a component we get with the trained model. Read [The Illustrated Word2vec](http://jalammar.github.io/illustrated-word2vec/) for a background on word embeddings.

![img](http://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png)

Note that the tokenizer does all these steps in a single line of code:

```python
tokenizer.encode("a visually stunning rumination on love", add_special_tokens=True)
```

Our input sentence is now the proper shape to be passed to DistilBERT.

If you’ve read [Illustrated BERT](http://jalammar.github.io/illustrated-bert/), this step can also be visualized in this manner:

![img](http://jalammar.github.io/images/distilBERT/bert-distilbert-input-tokenization.png)



###### Flowing Through DistilBERT

Passing the **input vector** through DistilBERT works [just like BERT](http://jalammar.github.io/illustrated-bert/). The **output** would be a **vector** for each input token. each vector is made up of 768 numbers (floats).

![img](http://jalammar.github.io/images/distilBERT/bert-model-input-output-1.png)

Because this is a **sentence classification task**, we ignore all except the first vector (the one associated with the [CLS] token). The one vector we pass as the input to the logistic regression model.

![img](http://jalammar.github.io/images/distilBERT/bert-model-calssification-output-vector-cls.png)

From here, it’s the logistic regression model’s job to **classify this vector** based on what it learned from its training phase. We can think of a prediction calculation as looking like this:

![img](http://jalammar.github.io/images/distilBERT/bert-distilbert-sentence-classification-example.png)

###### The Code

In this section we’ll highlight the code to train this sentence classification model. A notebook containing all this code is available on [colab](https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb) and [github](https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb).

Let’s play/start by importing the tools of the trade

More information see http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/

![img](http://jalammar.github.io/images/distilBERT/sst2-text-to-tokenized-ids-bert-example.png)

![img](http://jalammar.github.io/images/distilBERT/bert-input-tensor.png)

![img](http://jalammar.github.io/images/distilBERT/bert-distilbert-output-tensor-predictions.png)

![img](http://jalammar.github.io/images/distilBERT/bert-output-tensor.png)



###### **Recapping a sentence’s journey**

Each **row** is associated with a **sentence** from our dataset. To recap the processing path of the **first sentence**, we can think of it as looking like this:

![img](http://jalammar.github.io/images/distilBERT/bert-input-to-output-tensor-recap.png)

==那个数值是什么？？==



###### Slicing the important part

For **sentence classification**, we’re only **only** interested in BERT’s output for the [CLS] token, so we select that slice of the cube and discard everything else.

![img](http://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png)

This is how we slice that 3d tensor to get the 2d tensor we’re interested in:

```python
 # Slice the output for the first position for all the sequences, take all hidden unit outputs
features = last_hidden_states[0][:,0,:].numpy()
```

And now `features` is a 2d numpy array containing the **sentence embeddings** of all the sentences in our dataset.![img](http://jalammar.github.io/images/distilBERT/bert-output-cls-senteence-embeddings.png)



==What is embedding?==





###### [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional)

pre-trained represetation model 

 pre-trained BERT model can be fine-tuned with just one additional output layer to create SOTA models for a wide range of tasks, such as question answering and language inference

###### Methods used in the Paper: [softmax (output function)](https://paperswithcode.com/method/softmax)

<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giflrwiaamj31pi0dq0wb.jpg" alt="image-20200905111549311" style="zoom:50%;" />





##### **Bert Input Resresetation**

##### <img src="https://pic3.zhimg.com/80/v2-edafe957e971fe53feeadd2477706cf4_1440w.jpg" alt="img" style="zoom:50%;" />





